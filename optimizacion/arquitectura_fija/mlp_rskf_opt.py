import pandas as pd
import numpy as np
import keras_tuner as kt
import matplotlib.pyplot as plt
import os
import json
import random
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# =============================================================================
# 1. Cargar Datos desde Archivo
# =============================================================================
synthetic_df = pd.read_csv("synthetic_data.txt", sep="\t")
X = synthetic_df.drop("Mfalla", axis=1).values
y = synthetic_df["Mfalla"].values

# Codificar etiquetas
encoder = OneHotEncoder(sparse_output=False)
y_encoded = encoder.fit_transform(y.reshape(-1, 1))

# =============================================================================
# 2. Definir arquitectura de la DNN con hiperparÃ¡metros
# =============================================================================
def build_model(hp):
    model = Sequential()
    model.add(Dense(
        units=hp.Int("units_1", 32, 128, step=32),
        activation=hp.Choice("activation_1", ["relu", "tanh"]),
        input_shape=(X.shape[1],)
    ))
    model.add(Dropout(hp.Float("dropout_1", 0.2, 0.5, step=0.1)))
    
    model.add(Dense(
        units=hp.Int("units_2", 16, 64, step=16),
        activation=hp.Choice("activation_2", ["relu", "tanh"])
    ))
       
    model.add(Dense(len(np.unique(y)), activation="softmax"))
    
    model.compile(
        optimizer=Adam(learning_rate=hp.Choice("learning_rate", [0.01, 0.001, 0.0001])),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

# =============================================================================
# 3. ValidaciÃ³n Cruzada con K-Fold y KerasTuner
# =============================================================================
batch_sizes = [16, 32, 64, 128]  # ðŸ”¹ Lista de batch_sizes a evaluar
results = []  # ðŸ”¹ Para almacenar las mÃ©tricas de cada batch_size
hps_per_batch = {} # Diccionario para guardar hiperparÃ¡metros por batch size
histories_dict = {}  # Diccionario para guardar los histÃ³ricos de cada batch_size

for batch_size in batch_sizes:
    print(f"\nðŸ”¹ Entrenando con batch_size = {batch_size}")
    fold_accuracies = []
    fold_losses = []
    all_histories = []
    histories_dict[batch_size] = all_histories
    hps_per_fold = []
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]

        tuner = kt.RandomSearch(
            build_model,
            objective="val_accuracy",
            max_trials=20,
            executions_per_trial=3,
            directory=f"kf_kt_results_bs{batch_size}",
            project_name=f"mlp_rskf{fold}_bs{batch_size}"
            )   
    
        tuner.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val), verbose=0)
    
        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
        hps_per_fold.append(best_hps.values)
        
        early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
    
        best_model = tuner.hypermodel.build(best_hps)
        history = best_model.fit(X_train, y_train, epochs=100, batch_size=batch_size,
                             validation_data=(X_val, y_val), verbose=0, callbacks=[early_stop])
    
        all_histories.append(history)
    
        ruta = r"C:\Users\mario.adame\.spyder-py3\SECMF\modelos_neuronales\secmfe\kf_kt_train"
        best_model.save(os.path.join(ruta,(f"mlp_bm_kf{fold}_bs{batch_size}.keras")))
    
        fold_accuracies.append(history.history["val_accuracy"][-1])
        fold_losses.append(history.history["val_loss"][-1])
        
        histories_series = {batch_size : [hist.history for hist in histories] for batch_size, histories in histories_dict.items()}
        with open(os.path.join(ruta, f"mlp_bhf{fold}_bs{batch_size}_hy.json"), "w") as f:
            json.dump(histories_series, f, indent=4)
        
        hps_per_batch[batch_size] = hps_per_fold
        with open(os.path.join(ruta, f"mlp_bhf{fold}_bs{batch_size}.json"), "w") as f:
            json.dump(hps_per_batch, f, indent=4)
        
    # ðŸ”¹ Guardar resultados de este batch_size
    mean_acc = np.mean(fold_accuracies)
    std_acc = np.std(fold_accuracies)
    mean_loss = np.mean(fold_losses)
    std_loss = np.std(fold_losses)

    results.append({
        "batch_size": batch_size,
        "mean_accuracy": mean_acc,
        "std_accuracy": std_acc,
        "mean_loss": mean_loss,
        "std_loss": std_loss
    })

    print(f"ðŸ”¹ batch_size = {batch_size} â†’ Exactitud: {mean_acc:.5f} Â± {std_acc:.5f}, PÃ©rdida: {mean_loss:.5f} Â± {std_loss:.5f}")

# ðŸ”¹ Guardar resultados en JSON para anÃ¡lisis posterior
with open("mlp_batch_size_results.json", "w") as f:
    json.dump(results, f, indent=4)

# ðŸ”¹ Imprimir resumen
print("\nðŸ”¹ ComparaciÃ³n de batch_size:")
for r in results:
    print(f"batch_size = {r['batch_size']}: Exactitud = {r['mean_accuracy']:.5f} Â± {r['std_accuracy']:.5f}")

# =============================================================================
# 4. Entrenar Modelo Final con los Mejores HiperparÃ¡metros
# =============================================================================
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

epsilon = 1e-8
for batch_size in batch_sizes:
    print(f"\nðŸ”¹ Batch_size = {batch_size}")
    fold_accuracies = [hist.history["val_accuracy"][-1] for hist in histories_dict[batch_size]]
    fold_losses = [hist.history["val_loss"][-1] for hist in histories_dict[batch_size]]
    fold_std_acc = [np.std(hist.history["val_accuracy"]) for hist in histories_dict[batch_size]]
    fold_std_loss = [np.std(hist.history["val_loss"]) for hist in histories_dict[batch_size]]
    for fold, (acc, loss, std_acc, std_loss) in enumerate(zip(fold_accuracies, fold_losses, fold_std_acc, fold_std_loss)):
        score = acc / (loss + epsilon)
        print(f" Fold {fold}: SPS = {score:.5f}, Exactitud_val = {acc:.5f} Â± {std_acc:.5f}, PÃ©rdida_val = {loss:.5f} Â± {std_loss:.5f}")

# Seleccionar el mejor batch basado en el criterio SPS (Puntaje estabilidad - exactitud)
best_batch = max(results, key=lambda x: x["mean_accuracy"] / (x["mean_loss"] + + epsilon))["batch_size"]
# Imprimir el mejor batch y su puntaje SPS
best_result = next(r for r in results if r["batch_size"] == best_batch)
best_score = best_result["mean_accuracy"] / (best_result["mean_loss"] + epsilon)
print(f"\n El mejor batch_size encontrado es: {best_batch} con un SPS de {best_score:.5f}")
# Imprimir los valores de cada batch_size
print("\n ComparaciÃ³n de Batch Sizes:")
for r in results:
    score = r["mean_accuracy"] / (r["mean_loss"] + epsilon)
    print(f"Batch_size = {r['batch_size']}: SPS = {score:.5f}, " f"Exactitud = {r['mean_accuracy']:.5f} Â± {r['std_accuracy']:.5f}, " f"PÃ©rdida = {r['std_loss']:.5f}")
fold_accuracies = [hist.history["val_accuracy"][-1] for hist in histories_dict[best_batch]]
fold_losses = [hist.history["val_loss"][-1] for hist in histories_dict[best_batch]]
fold_std_acc = [np.std(hist.history["val_accuracy"]) for hist in histories_dict[best_batch]]
fold_std_loss = [np.std(hist.history["val_loss"]) for hist in histories_dict[best_batch]]

# ðŸ”¹ Mostrar los valores de cada fold dentro del batch Ã³ptimo antes de seleccionar el mejor
print(f"\n Resultados de los folds dentro del batch: {best_batch} Ã³ptimo:")
for fold, (acc, loss, std_acc, std_loss) in enumerate(zip(fold_accuracies, fold_losses, fold_std_acc, fold_std_loss)):
    score = acc / (loss + epsilon)
    print(f" Fold {fold}: SPS = {score:.5f}, Exactitud_val = {acc:.5f} Â± {std_acc:.5f}, PÃ©rdida_val = {loss:.5f} Â± {std_loss:.5f}")

best_fold_index = max(range(len(fold_accuracies)), key=lambda i: fold_accuracies[i] / (fold_losses[i] + epsilon))
best_fold_acc = fold_accuracies[best_fold_index]
best_fold_loss = fold_losses[best_fold_index]
best_fold_stda = fold_std_acc[best_fold_index]
best_fold_stdl = fold_std_loss[best_fold_index]
print(f"\n El mejor fold dentro del batch {best_batch} es: {best_fold_index}")

# ðŸ”¹ Obtener los hiperparÃ¡metros asociados al mejor fold
best_hps = hps_per_batch[best_batch][best_fold_index]
# ðŸ”¹ Guardar los hiperparÃ¡metros con su mÃ©trica
hps_ac_ls = [(best_hps, best_fold_acc / (best_fold_loss + epsilon))]
hps_ac_ls.sort(key=lambda x: x[1], reverse=True)
# ðŸ”¹ Imprimir los resultados finales
print("\n Resultados de la mejor combinaciÃ³n de hiperparÃ¡metros basado en SPS:")
for hps, score in hps_ac_ls:
    print(f"HiperparÃ¡metro: {hps}, Score: {score:.5f}")
# ðŸ”¹ Extraer los mejores hiperparÃ¡metros para construir el modelo
best_hyperparams = hps_ac_ls[0][0]
hp = kt.HyperParameters()
for key, value in best_hyperparams.items():
    hp.values[key] = value
    
final_model = build_model(hp)
    
final_model.compile(optimizer=Adam(learning_rate=best_hyperparams["learning_rate"]),
                    loss="categorical_crossentropy", metrics=["accuracy"])

early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
final_history = final_model.fit(X_train, y_train, epochs=100, batch_size=best_batch, validation_data=(X_val, y_val), verbose=0, callbacks=[early_stop])

final_model.save(os.path.join(ruta, "mlp_bm_gkv.keras"))
with open(os.path.join(ruta, "mlp_bm_hps.json"), "w") as f:
    json.dump(best_hyperparams, f, indent=4)
    
print("\nâœ… Entrenamiento del modelo final completado y guardado correctamente.")
# =============================================================================
# 5. VisualizaciÃ³n de resultados para los folds del mejor Batch Size
# =============================================================================
train_accuracies = [history.history["accuracy"][-1] for history in all_histories]
train_losses = [history.history["loss"][-1] for history in all_histories]

val_accuracies = fold_accuracies  # Exactitud de validaciÃ³n
val_losses = fold_losses  # PÃ©rdida de validaciÃ³n

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.errorbar(range(5), fold_accuracies, yerr=np.std(fold_accuracies), fmt='o', label='Exactitud')
plt.title(f"Exactitud en la mejor ValidaciÃ³n Cruzada (Batch {best_batch})")
plt.xlabel("Fold")
plt.ylabel("Exactitud")
plt.legend()

plt.subplot(1, 2, 2)
plt.errorbar(range(5), fold_losses, yerr=np.std(fold_losses), fmt='o', label='PÃ©rdida')
plt.title(f"PÃ©rdida en la mejor ValidaciÃ³n Cruzada (Batch {best_batch})")
plt.xlabel("Fold")
plt.ylabel("PÃ©rdida")
plt.legend()
plt.show()

# ðŸ”¹ VisualizaciÃ³n de la distribuciÃ³n de Exactitud por fold
# Calcular estadÃ­sticas de la caja
q1 = np.percentile(fold_accuracies, 25)  # Primer cuartil
q3 = np.percentile(fold_accuracies, 75)  # Tercer cuartil
mediana = np.median(fold_accuracies)  # Mediana
media = np.mean(fold_accuracies)
minimo = np.min(fold_accuracies)  # Valor mÃ­nimo
maximo = np.max(fold_accuracies)  # Valor mÃ¡ximo

plt.figure(figsize=(8, 5))

# Crear boxplot con bigotes visibles
box = plt.boxplot([fold_accuracies], labels=[f"CombinaciÃ³n mejores hiperparÃ¡metros del Batch {best_batch}"], patch_artist=True, showmeans=True, whis=1.5)

# Mostrar etiquetas numÃ©ricas en los valores clave
plt.text(0.92, mediana, f"{mediana:.5f}", verticalalignment='center', horizontalalignment='right', fontsize=10, color='red')
plt.text(0.92, media, f"{media:.5f}", verticalalignment='center', horizontalalignment='right', fontsize=10, color='green')
plt.text(1.08, q1, f"{q1:.5f}", verticalalignment='bottom', horizontalalignment='left', fontsize=10, color='blue')
plt.text(1.08, q3, f"{q3:.5f}", verticalalignment='top', horizontalalignment='left', fontsize=10, color='blue')
plt.text(0.95, minimo, f"{minimo:.5f}", verticalalignment='top', horizontalalignment='right', fontsize=10, color='black')
plt.text(0.95, maximo, f"{maximo:.5f}", verticalalignment='bottom', horizontalalignment='right', fontsize=10, color='black')

# ConfiguraciÃ³n del grÃ¡fico
plt.title(f"DistribuciÃ³n de la Exactitud en el mejor Batch: {best_batch}")
plt.ylabel("Exactitud")
plt.show()

# =============================================================================
# 6. VisualizaciÃ³n de Resultados para cada Fold
# =============================================================================
# Exactitud durante el entrenamiento del mejor modelo
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(final_history.history["accuracy"], label="Exactitud en entrenamiento")
plt.plot(final_history.history["val_accuracy"], label="Exactitud en validaciÃ³n")
plt.title("Exactitud del Mejor Modelo Global")
plt.xlabel("Ã‰pocas")
plt.ylabel("Exactitud")
plt.legend()

# PÃ©rdida durante el entrenamiento del mejor modelo
plt.subplot(1, 2, 2)
plt.plot(final_history.history["loss"], label="PÃ©rdida en entrenamiento")
plt.plot(final_history.history["val_loss"], label="PÃ©rdida en validaciÃ³n")
plt.title("PÃ©rdida del Mejor Modelo Global")
plt.xlabel("Ã‰pocas")
plt.ylabel("PÃ©rdida")
plt.legend()

plt.show()

for batch_size, histories in histories_dict.items():
    plt.figure(figsize=(12, 5))  # Crear nueva figura para cada batch_size
    
    # Subplot 1: Exactitud
    plt.subplot(1, 2, 1)
    for fold, history in enumerate(histories):
        plt.plot(history.history["accuracy"], label=f"Fold {fold} - Entrenamiento", alpha=0.6)
        plt.plot(history.history["val_accuracy"], label=f"Fold {fold} - ValidaciÃ³n", linestyle="dashed", alpha=0.6)

    plt.ylim(0, 1)
    plt.legend(ncol=2, fontsize=8)
    plt.title(f"Exactitud para Batch Size {batch_size}")
    plt.xlabel("Ã‰poca")
    plt.ylabel("Exactitud")

    # Subplot 2: PÃ©rdida
    plt.subplot(1, 2, 2)
    for fold, history in enumerate(histories):
        plt.plot(history.history["loss"], label=f"Fold {fold} - Entrenamiento", alpha=0.6)
        plt.plot(history.history["val_loss"], label=f"Fold {fold} - ValidaciÃ³n", linestyle="dashed", alpha=0.6)

    plt.legend(ncol=2, fontsize=8)
    plt.title(f"PÃ©rdida para Batch Size {batch_size}")
    plt.xlabel("Ã‰poca")
    plt.ylabel("PÃ©rdida")

    plt.show()  # Mostrar la figura para este batch_size antes de pasar al siguiente

# SÃ­mbolos para cada batch size
symbols = {16: "o", 32: "s", 64: "^", 128: "D"}
colors_train = "blue"  # Color de entrenamiento
colors_val = "orange"  # Color de validaciÃ³n

# Almacenar estadÃ­sticas por fold
batch_sizes = [16, 32, 64, 128]
folds = len(next(iter(histories_dict.values())))  # NÃºmero de folds

# Crear estructuras para almacenar datos
metrics = ["Exactitud Entrenamiento", "Exactitud ValidaciÃ³n", "PÃ©rdida Entrenamiento", "PÃ©rdida ValidaciÃ³n"]
values = {metric: {batch_size: [] for batch_size in batch_sizes} for metric in metrics}

# Extraer valores por fold y batch size
for batch_size, histories in histories_dict.items():
    for fold, history in enumerate(histories):
        values["Exactitud Entrenamiento"][batch_size].append(history.history["accuracy"][-1])
        values["Exactitud ValidaciÃ³n"][batch_size].append(history.history["val_accuracy"][-1])
        values["PÃ©rdida Entrenamiento"][batch_size].append(history.history["loss"][-1])
        values["PÃ©rdida ValidaciÃ³n"][batch_size].append(history.history["val_loss"][-1])

# Crear la figura con dos subplots
fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=False)

# Espaciado dentro de cada fold para los puntos
x_offsets = np.linspace(-0.2, 0.2, len(batch_sizes))

# Graficar cada mÃ©trica en su respectivo subplot
for i, (ax, (metric_train, metric_val), ylabel) in enumerate(zip(
    axes,
    [("Exactitud Entrenamiento", "Exactitud ValidaciÃ³n"), ("PÃ©rdida Entrenamiento", "PÃ©rdida ValidaciÃ³n")],
    ["Exactitud", "PÃ©rdida"]
)):
    for fold in range(folds):
        x_train = fold + x_offsets - 0.3  # Desplazamiento para entrenamiento
        x_val = fold + x_offsets + 0.3    # Desplazamiento para validaciÃ³n

        # Sombreado para diferenciar los folds
        if fold % 2 == 0:
            ax.axvspan(fold - 0.5, fold + 0.5, color="gray", alpha=0.2)

        for j, batch_size in enumerate(batch_sizes):
            # Entrenamiento
            ax.scatter(x_train[j], values[metric_train][batch_size][fold],
                       marker=symbols[batch_size], color=colors_train, edgecolor="black",
                       label=f"Batch {batch_size}" if fold == 0 else "")
            # ValidaciÃ³n
            ax.scatter(x_val[j], values[metric_val][batch_size][fold],
                       marker=symbols[batch_size], color=colors_val, edgecolor="black")

    # ConfiguraciÃ³n del subplot
    ax.set_xticks(range(folds))
    ax.set_xticklabels([f'Fold {i+1}' for i in range(folds)], fontsize=14)
    ax.set_xlabel("Folds", fontsize=14)
    ax.set_title(f"DispersiÃ³n de {ylabel}", fontsize=16)
    ax.set_ylabel(ylabel, fontsize=14)
    ax.grid(True, linestyle="--", alpha=0.6)

# Agregar leyenda al primer subplot
axes[0].legend(fontsize=12)
plt.tight_layout()
plt.show()

# Definir sÃ­mbolos y colores para cada batch size
symbols = {16: "o", 32: "s", 64: "^", 128: "D"}
colors = {16: "black", 32: "black", 64: "black", 128: "black"}

# Definir mÃ©tricas y estructura de datos
metrics = ["Exactitud Entrenamiento", "Exactitud ValidaciÃ³n", "PÃ©rdida Entrenamiento", "PÃ©rdida ValidaciÃ³n"]
means = {metric: [] for metric in metrics}
medians = {metric: [] for metric in metrics}
stds = {metric: [] for metric in metrics}
batch_sizes = []

# Calcular estadÃ­sticas para cada batch size
for batch_size, histories in histories_dict.items():
    final_acc_train = [history.history["accuracy"][-1] for history in histories]
    final_acc_val = [history.history["val_accuracy"][-1] for history in histories]
    final_loss_train = [history.history["loss"][-1] for history in histories]
    final_loss_val = [history.history["val_loss"][-1] for history in histories]

    means["Exactitud Entrenamiento"].append(np.mean(final_acc_train))
    medians["Exactitud Entrenamiento"].append(np.median(final_acc_train))
    stds["Exactitud Entrenamiento"].append(np.std(final_acc_train))

    means["Exactitud ValidaciÃ³n"].append(np.mean(final_acc_val))
    medians["Exactitud ValidaciÃ³n"].append(np.median(final_acc_val))
    stds["Exactitud ValidaciÃ³n"].append(np.std(final_acc_val))

    means["PÃ©rdida Entrenamiento"].append(np.mean(final_loss_train))
    medians["PÃ©rdida Entrenamiento"].append(np.median(final_loss_train))
    stds["PÃ©rdida Entrenamiento"].append(np.std(final_loss_train))

    means["PÃ©rdida ValidaciÃ³n"].append(np.mean(final_loss_val))
    medians["PÃ©rdida ValidaciÃ³n"].append(np.median(final_loss_val))
    stds["PÃ©rdida ValidaciÃ³n"].append(np.std(final_loss_val))

    batch_sizes.append(batch_size)

# Crear figura con 4 subplots (2x2)
fig, axes = plt.subplots(2, 2, figsize=(14, 12))
titles = ["Exactitud Entrenamiento", "Exactitud ValidaciÃ³n", "PÃ©rdida Entrenamiento", "PÃ©rdida ValidaciÃ³n"]
y_labels = ["Exactitud", "Exactitud", "PÃ©rdida", "PÃ©rdida"]

# Iterar sobre cada mÃ©trica y asignarla a su subplot correspondiente
for idx, (metric, ax) in enumerate(zip(metrics, axes.flatten())):
    for i, batch_size in enumerate(batch_sizes):
        mean_value = means[metric][i]
        median_value = medians[metric][i]
        std_value = stds[metric][i]

        # Graficar con barras de error
        ax.errorbar(i, mean_value, yerr=std_value, fmt=symbols[batch_size], 
                    markersize=8, markerfacecolor=colors[batch_size], markeredgecolor="black",
                    capsize=5, label=f"Batch {batch_size}" if idx == 0 else "")

        # Ajustar posiciÃ³n de las etiquetas para que no se solapen
        text_offset = (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.02  # 2% del rango del eje Y
        ax.text(i, mean_value + text_offset, f"Î¼={mean_value:.5f}\nMd={median_value:.5f}", 
                ha='center', fontsize=9, color="blue", bbox=dict(facecolor='white', alpha=0.7))
        ax.text(i, mean_value - text_offset * 2, f"Ïƒ={std_value:.5f}", 
                ha='center', fontsize=9, color="green")

    # Configurar ejes
    ax.set_xticks(range(len(batch_sizes)))
    ax.set_xticklabels([f"Batch {bs}" for bs in batch_sizes])
    ax.set_ylabel(y_labels[idx])
    ax.set_title(titles[idx])
    ax.grid(True, linestyle="--", alpha=0.6)

    # Agregar leyenda solo en el primer grÃ¡fico
    if idx == 0:
        ax.legend()

plt.tight_layout()
plt.show()

# =============================================================================
# 7. ImpresiÃ³n de Precisiones, PÃ©rdidas, e HiperparÃ¡metros
# =============================================================================
print(f"Media de Exactitud en validaciÃ³n cruzada: {np.mean(fold_accuracies):.5f} Â± {np.std(fold_accuracies):.5f}")
print(f"Media de pÃ©rdida en validaciÃ³n cruzada: {np.mean(fold_losses):.5f} Â± {np.std(fold_losses):.5f}")
print(f"Media de Exactitud en entrenamiento: {np.mean(train_accuracies):.5f} Â± {np.std(train_accuracies):.5f}")
print(f"Media de pÃ©rdida en entrenamiento: {np.mean(train_losses):.5f} Â± {np.std(train_losses):.5f}")
print(f"Media de Exactitud en validaciÃ³n: {np.mean(best_fold_acc):.5f} Â± {np.std(best_fold_acc):.5f}")
print(f"Media de pÃ©rdida en validaciÃ³n: {np.mean(best_fold_loss):.5f} Â± {np.std(best_fold_loss):.5f}")
print(f"Mejores hiperparÃ¡metros del batch_size con mejor desempeÃ±o: batch_size:{best_batch} ", best_hyperparams)


