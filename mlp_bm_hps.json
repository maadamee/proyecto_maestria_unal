{
    "num_layers": 1,
    "units_1": 128,
    "activation_1": "relu",
    "dropout_1": 0.4,
    "learning_rate": 0.01
}